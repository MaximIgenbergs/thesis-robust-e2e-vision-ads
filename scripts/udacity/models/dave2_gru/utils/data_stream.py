from __future__ import annotations

"""
Sequence dataset utilities for DAVE2-GRU (tf.data-based).

Generated by ChatGPT.
"""

import json
import os
from typing import Iterator

import numpy as np
from PIL import Image
import tensorflow as tf  # type: ignore


# JSON and image helpers

def load_json(path: str) -> dict:
    with open(path, "rt", encoding="utf-8") as f:
        return json.load(f)


def json_path_for(image_path: str) -> str:
    base = os.path.basename(image_path)
    idx = os.path.splitext(base)[0].split("_")[-1]
    return os.path.join(os.path.dirname(image_path), f"record_{idx}.json")


def frame_index(image_path: str, meta: dict) -> int:
    for key in ("meta/frame", "frame", "frame_index"):
        if key in meta:
            try:
                return int(meta[key])
            except Exception:
                pass

    stem = os.path.splitext(os.path.basename(image_path))[0]
    try:
        return int(stem.split("_")[-1])
    except Exception:
        return -1


def track_id(image_path: str, meta: dict) -> int:
    for key in ("meta/track_id", "track_id"):
        if key in meta:
            try:
                return int(meta[key])
            except Exception:
                pass

    parent = os.path.basename(os.path.dirname(image_path))
    if parent.startswith("track_"):
        try:
            return int(parent.split("_", 1)[1])
        except Exception:
            pass

    return 0


def find_images(data_dir: str) -> list[str]:
    data_dir = os.path.expanduser(data_dir)
    out: list[str] = []
    for root, _, files in os.walk(data_dir):
        for name in files:
            if name.lower().endswith(".jpg"):
                out.append(os.path.join(root, name))
    out.sort()
    return out


# Index by track (paths only; no pixels in RAM)

def index_by_track(data_dir: str) -> dict[int, list[tuple[int, str, str]]]:
    """
    Build an index of images and JSONs grouped by track_id.

    Returns:
        {track_id: [(frame_idx, image_path, json_path), ...]} sorted by frame_idx.
    """
    images = find_images(data_dir)
    per_track: dict[int, list[tuple[int, str, str]]] = {}

    for img in images:
        js_path = json_path_for(img)
        if not os.path.exists(js_path):
            continue

        meta = load_json(js_path)
        tid = track_id(img, meta)
        idx = frame_index(img, meta)
        per_track.setdefault(tid, []).append((idx, img, js_path))

    for tid in per_track:
        per_track[tid].sort(key=lambda t: t[0])

    return per_track


def temporal_split_single_track(
    per_track: dict[int, list[tuple[int, str, str]]],
    seq_len: int,
    val_split: float,
) -> tuple[dict[int, list[tuple[int, str, str]]], list[int], list[int]]:
    """
    Temporal split when there is exactly one track.

    Train: first (1 - val_split) frames
    Val:   last  val_split  frames

    Both sides must have at least seq_len frames; otherwise we keep all as train.
    """
    tids = sorted(per_track.keys())
    assert len(tids) == 1, "temporal_split_single_track expects exactly one track"

    tid = tids[0]
    frames = per_track[tid]
    n = len(frames)

    if n < 2 * seq_len:
        return per_track, [tid], []

    n_val_frames = max(seq_len, int(round(n * val_split)))
    n_val_frames = min(n - seq_len, n_val_frames)
    cut = n - n_val_frames

    per_track_new: dict[int, list[tuple[int, str, str]]] = {
        0: frames[:cut],
        1: frames[cut:],
    }
    return per_track_new, [0], [1]


def split_tracks(
    per_track: dict[int, list[tuple[int, str, str]]],
    val_split: float,
    seed: int = 42,
) -> tuple[list[int], list[int]]:
    """
    Multi-track split: shuffle track IDs and put ~val_split fraction into validation.
    """
    tids = sorted(per_track.keys())
    if not tids:
        return [], []

    if len(tids) == 1:
        return [], tids

    import random

    rng = random.Random(seed)
    rng.shuffle(tids)

    n_val = int(round(len(tids) * val_split))
    n_val = max(1, n_val)
    n_val = min(n_val, len(tids) - 1)

    return tids[:-n_val], tids[-n_val:]


def count_sequences(n_frames: int, seq_len: int, stride: int) -> int:
    if n_frames < seq_len:
        return 0
    return (n_frames - seq_len) // stride + 1


def total_sequences(
    per_track: dict[int, list[tuple[int, str, str]]],
    tids: list[int],
    seq_len: int,
    stride: int,
) -> int:
    return sum(count_sequences(len(per_track[tid]), seq_len, stride) for tid in tids)


# Image/label loading per window

def load_image(path: str, out_hw: tuple[int, int]) -> np.ndarray:
    h, w = out_hw
    im = Image.open(path)
    if im.size != (w, h):
        im = im.resize((w, h), resample=Image.BILINEAR)
    arr = np.asarray(im, dtype=np.float32)
    if arr.ndim == 2:
        arr = np.stack([arr] * 3, axis=-1)
    return arr


def read_label(json_path: str, num_outputs: int) -> np.ndarray:
    meta = load_json(json_path)
    steer = float(meta.get("user/angle", meta.get("user/angel", 0.0)))
    throttle = float(meta.get("user/throttle", 0.0))

    if num_outputs == 1:
        return np.asarray([steer], dtype=np.float32)

    return np.asarray([steer, throttle], dtype=np.float32)


# Python generator that yields unbatched windows

def iter_windows(
    per_track: dict[int, list[tuple[int, str, str]]],
    tids: list[int],
    seq_len: int,
    stride: int,
    out_hw: tuple[int, int],
    num_outputs: int,
) -> Iterator[tuple[np.ndarray, np.ndarray]]:
    for tid in tids:
        frames = per_track[tid]
        n = len(frames)
        if n < seq_len:
            continue

        for start in range(0, n - seq_len + 1, stride):
            window = frames[start:start + seq_len]
            images = [load_image(img, out_hw) for (_, img, __) in window]

            _, _, last_json = window[-1]
            y = read_label(last_json, num_outputs)
            x = np.stack(images, axis=0)

            yield x, y


# tf.data wrappers

def make_sequence_dataset(
    per_track: dict[int, list[tuple[int, str, str]]],
    tids: list[int],
    seq_len: int,
    stride: int,
    out_shape: tuple[int, int, int],
    num_outputs: int,
    batch_size: int,
    repeat: bool = False,
) -> tf.data.Dataset:
    h, w, c = out_shape

    def generator() -> Iterator[tuple[np.ndarray, np.ndarray]]:
        return iter_windows(per_track, tids, seq_len, stride, (h, w), num_outputs)

    ds = tf.data.Dataset.from_generator(
        generator,
        output_signature=(
            tf.TensorSpec(shape=(seq_len, h, w, c), dtype=tf.float32),
            tf.TensorSpec(shape=(num_outputs,), dtype=tf.float32),
        ),
    )

    if repeat:
        ds = ds.repeat()

    return ds.batch(batch_size, drop_remainder=False).prefetch(tf.data.AUTOTUNE)
